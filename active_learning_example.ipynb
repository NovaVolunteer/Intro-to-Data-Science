{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1iP_CsgATmdqNVQDTW2Z5V25IyEVRz4dj",
      "authorship_tag": "ABX9TyNwJTzQcRa8V9A3NJ8XrFs5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NovaVolunteer/Intro-to-Data-Science/blob/master/active_learning_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Active learning is a powerful technique in machine learning that optimizes the labeling process by selecting the most informative data points for annotation. For example, in a manufacturing setting, a company may develop an image classification model to detect defective products on an assembly line. Instead of labeling a vast dataset upfront, the company employs an active learning approach, starting with a small, labeled dataset to train an initial model. The trained model then evaluates a large pool of unlabeled product images and, using an uncertainty sampling strategy, selects the images where it is least confident. These selected images are sent to human annotators for labeling, and the newly labeled data is incorporated into the training set for model retraining. This process repeats iteratively, allowing the model to improve its accuracy while minimizing the number of labeled examples required. Active learning significantly reduces annotation costs, enhances model efficiency by focusing on difficult cases, and accelerates the learning process, making it a practical approach for scenarios where data labeling is expensive or time-consuming. Below is a example script (kinda sudo code) for this scenerio."
      ],
      "metadata": {
        "id": "VF5zG-BiXC26"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCBGC6m1XCNt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Placeholder functions for loading data and model\n",
        "def load_initial_labeled_data():\n",
        "    \"\"\"Loads a small initial set of labeled training data.\"\"\"\n",
        "    X_train = np.random.rand(100, 64, 64, 3)  # 100 sample images (64x64 RGB)\n",
        "    y_train = np.random.randint(0, 2, 100)  # Binary classification (0: good, 1: defective)\n",
        "    return X_train, y_train\n",
        "\n",
        "def load_unlabeled_data():\n",
        "    \"\"\"Loads a pool of unlabeled images.\"\"\"\n",
        "    return np.random.rand(1000, 64, 64, 3)  # 1000 unlabeled images\n",
        "\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"Trains a model on labeled data. Placeholder for actual model training.\"\"\"\n",
        "    model = \"Trained Model Placeholder\"\n",
        "    return model\n",
        "\n",
        "def predict_uncertainty(model, X_unlabeled):\n",
        "    \"\"\"Simulates model predictions and computes uncertainty scores.\"\"\"\n",
        "    uncertainty_scores = np.random.rand(len(X_unlabeled))  # Random uncertainties\n",
        "    return uncertainty_scores\n",
        "\n",
        "def query_most_uncertain_samples(X_unlabeled, uncertainty_scores, num_samples=10):\n",
        "    \"\"\"Selects the most uncertain samples for labeling.\"\"\"\n",
        "    most_uncertain_indices = np.argsort(uncertainty_scores)[-num_samples:]  # Highest uncertainty\n",
        "    return most_uncertain_indices\n",
        "\n",
        "def human_annotation(X_selected):\n",
        "    \"\"\"Simulates human annotation of selected images.\"\"\"\n",
        "    return np.random.randint(0, 2, len(X_selected))  # Random labels\n",
        "\n",
        "# Step 1: Load initial labeled dataset and pool of unlabeled data\n",
        "X_train, y_train = load_initial_labeled_data()\n",
        "X_unlabeled = load_unlabeled_data()\n",
        "\n",
        "# Active Learning Loop\n",
        "for iteration in range(5):  # Perform 5 rounds of active learning\n",
        "    print(f\"Active Learning Iteration {iteration + 1}\")\n",
        "\n",
        "    # Step 2: Train the model on the labeled data\n",
        "    model = train_model(X_train, y_train)\n",
        "\n",
        "    # Step 3: Predict uncertainty scores for unlabeled data\n",
        "    uncertainty_scores = predict_uncertainty(model, X_unlabeled)\n",
        "\n",
        "    # Step 4: Select most uncertain samples for labeling\n",
        "    uncertain_indices = query_most_uncertain_samples(X_unlabeled, uncertainty_scores, num_samples=10)\n",
        "    X_selected = X_unlabeled[uncertain_indices]\n",
        "\n",
        "    # Step 5: Human annotates selected samples\n",
        "    y_selected = human_annotation(X_selected)\n",
        "\n",
        "    # Step 6: Add newly labeled data to the training set\n",
        "    X_train = np.concatenate([X_train, X_selected], axis=0)\n",
        "    y_train = np.concatenate([y_train, y_selected], axis=0)\n",
        "\n",
        "    # Remove labeled samples from the unlabeled pool\n",
        "    X_unlabeled = np.delete(X_unlabeled, uncertain_indices, axis=0)\n",
        "\n",
        "print(\"Active Learning Process Complete!\")\n"
      ]
    }
  ]
}